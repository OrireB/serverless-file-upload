# ðŸ“ Serverless File Upload System with AWS

A simple, fully serverless system that allows users to upload files to an S3 bucket, stores file metadata in DynamoDB, and lets you view the metadata via an API endpoint or through a simple HTML page.

---

## ðŸš€ Features
- ðŸ“¦ Upload files to **S3**
- âš™ï¸ Automatically triggers **Lambda**
- ðŸ“ Saves metadata (filename, timestamp) to **DynamoDB**
- ðŸŒ Fetch metadata via **API Gateway**
- ðŸ’» (Optional) HTML page to view uploaded file metadata

---

## ðŸ›  Technologies Used
- **Amazon S3** â€“ File storage
- **AWS Lambda** â€“ Serverless compute
- **Amazon DynamoDB** â€“ Metadata storage
- **API Gateway** â€“ Public API access
- **Boto3 (Python SDK)** â€“ Used in Lambda
- **HTML + JavaScript** â€“ Optional UI to view metadata

---

## ðŸ§± Project Architecture Overview

You'll create:

1. âœ… S3 Bucket â€” For file uploads.
2. âœ… DynamoDB Table â€” Stores metadata (filename, timestamp).
3. âœ… Lambda Function 1 â€” Stores file metadata from S3 to DynamoDB.
4. âœ… S3 Trigger â€” Invokes the Lambda on file upload.
5. âœ… API Gateway + Lambda â€” Exposes metadata via a public REST API.
6. âœ… (Optional) HTML Viewer â€” A simple web page that fetches and displays file metadata.

---

## ðŸ“‚ Step-by-Step Setup

### ðŸ”¹ Step 1: Create an S3 Bucket

> The S3 bucket is where your users (or scripts) will upload files. These uploads will automatically trigger the Lambda function.

1. Go to AWS Console â†’ **S3 > Create Bucket**
2. Name: `success-guaranteed` (or a unique name)
3. Leave default settings
4. Click **Create Bucket**

**Or create via code:**

[create_s3_bucket.py](https://github.com/OrireB/serverless-file-upload/commit/72346e7604f417096312fa3238fc8597df41d6ba#diff-5ff7df0666e13dc29b7602c447b4fcb4ce476ba1537778e5153d80a8f277b799) for working with AWS services in Python.

---

### ðŸ”¹ Step 2: Create a DynamoDB Table
> This table will store metadata about uploaded files: filename and upload timestamp.

1. Go to AWS Console â†’ DynamoDB â†’ Tables â†’ Create Table
2. Table Name: FileMetadata
3. Partition Key:
Name: id
Type: String
4. Capacity Mode: On-Demand
5. Advanced Settings:
No changes needed for default access settings unless youâ€™re restricting access.
6. Click Create Table

---

### ðŸ”¹ Step 3: Create Lambda Function â€“ MyStoreFileMetadata
> This function is triggered by the S3 bucket and saves metadata to DynamoDB.

1. Go to AWS Console â†’ Lambda â†’ Create Function
2. Choose: **Author from scratch**
3. Function name: MyStoreFileMetadata
4. Runtime: Python 3.13
5. Architecture: x86_64
6. Permissions: **Create a new role with basic Lambda permissions**
7. Click â€œCreate Functionâ€
7. Replace the default code with:

import boto3
import json
from datetime import datetime

dynamodb = boto3.resource('dynamodb')
table = dynamodb.Table('FileMetadata')

def lambda_handler(event, context):
    for record in event['Records']:
        filename = record['s3']['object']['key']
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        table.put_item(Item={
            'id': filename,
            'filename': filename,
            'timestamp': timestamp
        })
    return {
        'statusCode': 200,
        'body': json.dumps('Metadata stored successfully.')
    }

8. Click **Deploy**

This will add DynamoDB Read Permissions
1. Go to: AWS Console â†’ IAM â†’ Roles
2. Find the IAM role created for MyStoreFileMetadata
3. Click the role â†’ Add permissions â†’ Attach policies
4. Search and attach:
* AmazonDynamoDBReadOnlyAccess (for simplicity, or create a custom policy later)
* You can replace this with a more restricted policy if needed, but for the project full access is fine.

---

### ðŸ”¹ Step 4: Add S3 Trigger to Lambda
> This ensures the Lambda runs every time a file is uploaded to your S3 bucket.

1. Go to: AWS Console â†’ Lambda â†’ Function
2. Click the **â€œ+ Add triggerâ€** button.
3. Choose:
* Trigger configuration: Select S3
* Bucket: **Select success-guaranteed**
* Event type: Choose **PUT** (this means: when a file is uploaded)
3. Recursive invocation: Check the confirmation box
4. Click **Add**

---

### ðŸ”¹ Step 5: Create Lambda Function â€“ GetFileMetadata
> Create an API Gateway + Lambda to Fetch Metadata
The goal:
> Youâ€™ll create a new Lambda function that will read all file metadata from DynamoDB and return it via an API Gateway endpoint (so you can access it with a browser or curl).

1. Go to AWS Console â†’ Lambda â†’ Create Function
2. Choose: **Author from scratch**
3. Function name: GetFileMetadata
4. Runtime: Python 3.13
5. Architecture: x86_64
6. Permissions: **Create a new role with basic Lambda permissions**
7. Click â€œCreate Functionâ€
7. Replace the default code with:

import boto3
import json

dynamodb = boto3.resource('dynamodb')
table = dynamodb.Table('FileMetadata')

def lambda_handler(event, context):
    try:
        response = table.scan()
        return {
            'statusCode': 200,
            'body': json.dumps(response['Items'])
        }
    except Exception as e:
        return {
            'statusCode': 500,
            'body': json.dumps(str(e))
        }

8. Click **Deploy**
9. Go to **IAM** > **Roles** â†’ find this Lambdaâ€™s role â†’ **Add policy**: AmazonDynamoDBReadOnlyAccess

---

### ðŸ”¹ Step 6: Create REST API with API Gateway
> Exposes the Lambda above via a URL so it can be called from a browser or HTML file.

1. Go to AWS Console â†’ API Gateway â†’ Create API
2. Choose: REST API â†’ Build
3. API Name: FileMetadataAPI
4. Endpoint Type: Keep as Regional
5. IP address type: IPv4
6. Click **Create API**

## Add Resource and Method
1. Under /, click Actions â†’ Create Resource
Resource Name: metadata
Resource Path: /metadata (auto-filled)
CORS (Cross Origin Resource Sharing) Info â†’ Click âœ… checkmark
Click **Create Resource**

## Create a GET Method
2. Select /metadata â†’ Actions â†’ Create Method
Choose GET from the dropdown â†’ Click âœ… checkmark
* Configure the method:
     ** Integration type: Lambda Function
     ** Use Lambda Proxy integration: âœ… checked
     ** Lambda Function Name: GetFileMetadata
* Click **Save**

## Deploy the API
Click â€œActionsâ€ â†’ â€œDeploy APIâ€
* Deployment stage:
    ** Choose [New Stage]
    ** Stage name: prod
* Click Deploy

After deployment, note the Invoke URL
 Exanple:
https://u2shohuema.execute-api.us-east-1.amazonaws.com/prod

---

### ðŸ”¹ Step 7: Upload a File to S3 for Testing
> Uploading any file will trigger Lambda and populate DynamoDB.
When you upload a file like Fierce.jpeg to the success-guaranteed bucket, AWS will:

1.Trigger your Lambda function
2. Lambda will store file metadata in DynamoDB table FileMetadata

Example Python script:
import boto3

client = boto3.client('s3')

client.upload_file(
    Filename='Fierce.jpeg',
    Bucket='success-guaranteed',
    Key='Fierce.jpeg'
)

Go to DynamoDB â†’ FileMetadata â†’ Check if metadata is stored.

---

### ðŸ”¹ Step 8: Create and Host HTML Viewer (Optional but Recommended)
> This web page makes it easy to view metadata without using Postman or the browser console.

##Create HTML File
Save the code below as file-metadata.html in your VS Code or text editor:

---

##Upload and Host HTML in S3
1. Upload the file-metadata.html to your success-guaranteed bucket
2. Enable Static Website Hosting under bucket Properties
3. Set:
Index document: file-metadata.html
4. Make the file public (Actions > Make public)
5. Open the S3 website URL to view your HTML page live

---

###ðŸ§ª Expected Output
If all is configured correctly:
Now,
Test the Full URL in the Browser
1. Paste this in your browser:
https://abc123.execute-api.us-east-1.amazonaws.com/prod/metadata

2. If everything is set up correctly, youâ€™ll see:
* A JSON array with metadata from your DynamoDB table
You should see a JSON response like:

```python
[
  {
    "id": "Fierce.jpeg",
    "filename": "Fierce.jpeg",
    "timestamp": "2025-06-18 18:30:00"
  }
]

---

###ðŸ§¹ Cleanup Checklist (Avoid Unexpected Charges)
1. ðŸ—‘ Delete S3 Bucket (after emptying it)
2. ðŸ—‘ Delete both Lambda functions
3. ðŸ—‘ Delete DynamoDB table FileMetadata
4. ðŸ—‘ Delete API Gateway
5. ðŸ—‘ Delete IAM roles (if not reused)
6. ðŸ—‘ Delete CloudWatch logs (optional)

---

###âœ… Final Tip
Always double-check your AWS Billing Dashboard to ensure no lingering services are active that may incur charges.
